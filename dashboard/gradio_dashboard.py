# dashboard/gradio_dashboard.py

import gradio as gr
from transformers import pipeline
import requests
from typing import Dict, List, Any
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ────── 1. Set up an open-source summarization LLM (Flan-T5-Small) ──────
try:
    summarizer_hf = pipeline(
        "summarization",
        model="google/flan-t5-small",
        device=-1,  # CPU only
        framework="pt",  # PyTorch
        max_length=50,  # Reduced to avoid warnings
        min_length=10,  # Reduced proportionally
        do_sample=False,
    )
    logger.info("LLM pipeline initialized successfully")
except Exception as e:
    logger.error(f"Error initializing LLM: {e}")
    summarizer_hf = None


# Simple summarization function to replace LangChain
def summarize_text(text: str, context: str = "") -> str:
    """Simple text summarization function"""
    if summarizer_hf is not None:
        try:
            # Truncate input if too long
            max_input_length = 200
            if len(text) > max_input_length:
                text = text[:max_input_length] + "..."

            result = summarizer_hf(text, max_length=30, min_length=10, do_sample=False)
            summary = result[0]['summary_text']
            return f"{context}: {summary}" if context else summary
        except Exception as e:
            logger.error(f"Error in summarization: {e}")
            return f"{context}: Error generating summary - {str(e)[:100]}..."
    else:
        return f"{context}: Mock summary for: {text[:100]}..."


# ────── 2. Tools for MCP Endpoints ──────

class DetectDriftTool:
    def __init__(self):
        self.name = "detect_drift"
        self.description = "Detect semantic drift for a given agent over a time window."

    def run(self, tool_input: Dict[str, Any]) -> Dict[str, Any]:
        try:
            resp = requests.post("http://localhost:6002/detect_drift", json=tool_input, timeout=5)
            resp.raise_for_status()
            return resp.json()
        except Exception as e:
            logger.error(f"Error detecting drift: {e}")
            return {
                "drift_score": 0.15,
                "status": "normal",
                "details": "Fallback data - service unavailable",
                "timestamp": "2025-06-03T19:30:00Z"
            }


class ListVersionsTool:
    def __init__(self):
        self.name = "list_versions"
        self.description = "Retrieve the list of stored agent versions."

    def run(self, tool_input: Dict[str, Any] = None) -> List[str]:
        try:
            resp = requests.get("http://localhost:6003/list_versions", timeout=5)
            resp.raise_for_status()
            return resp.json()
        except Exception as e:
            logger.error(f"Error listing versions: {e}")
            return ["v1.0.0", "v1.1.0", "v2.0.0", "v2.1.0"]  # Fallback versions


class CompareVersionsTool:
    def __init__(self):
        self.name = "compare_versions"
        self.description = "Compare two agent versions and return metric diffs."

    def run(self, tool_input: Dict[str, Any]) -> Dict[str, Any]:
        try:
            resp = requests.post("http://localhost:6003/compare_versions", json=tool_input, timeout=5)
            resp.raise_for_status()
            return resp.json()
        except Exception as e:
            logger.error(f"Error comparing versions: {e}")
            return {
                "metric_differences": {
                    "accuracy": 0.035,
                    "latency": -0.12,
                    "memory_usage": 0.08,
                    "throughput": 0.05
                },
                "summary": "Fallback comparison data"
            }


class RunTestsTool:
    def __init__(self):
        self.name = "run_tests"
        self.description = "Run all tests on a given agent version."

    def run(self, tool_input: Dict[str, Any]) -> Dict[str, str]:
        try:
            tests = requests.get("http://localhost:6001/list_tests", timeout=5).json()
            results = {}
            for test_name in tests:
                payload = {"test_name": test_name, "agent_version": tool_input["agent_version"]}
                r = requests.post("http://localhost:6001/run_test", json=payload, timeout=10)
                r.raise_for_status()
                results[test_name] = r.json()["status"]
            return results
        except Exception as e:
            logger.error(f"Error running tests: {e}")
            return {
                "unit_tests": "passed",
                "integration_tests": "passed",
                "performance_tests": "warning",
                "security_tests": "passed",
                "regression_tests": "failed"
            }


class TriggerRetrainTool:
    def __init__(self):
        self.name = "trigger_retraining"
        self.description = "Trigger retraining via Modal."

    def run(self, tool_input: Dict[str, Any]) -> Dict[str, str]:
        try:
            resp = requests.post("http://localhost:6004/trigger_retraining", json=tool_input, timeout=5)
            resp.raise_for_status()
            return resp.json()
        except Exception as e:
            logger.error(f"Error triggering retraining: {e}")
            job_id = f"retrain_job_{hash(str(tool_input)) % 10000}"
            return {"job_id": job_id, "status": "initiated", "message": "Mock retraining job started"}


class CheckRetrainStatusTool:
    def __init__(self):
        self.name = "check_retraining_status"
        self.description = "Check the status of an ongoing retraining job."

    def run(self, tool_input: Dict[str, Any]) -> Dict[str, str]:
        try:
            resp = requests.post("http://localhost:6004/check_retraining_status", json=tool_input, timeout=5)
            resp.raise_for_status()
            return resp.json()
        except Exception as e:
            logger.error(f"Error checking retraining status: {e}")
            return {
                "status": "running",
                "progress": "65%",
                "estimated_completion": "15 minutes",
                "current_stage": "model_validation"
            }


# ────── 3. Initialize Tools ──────
drift_tool = DetectDriftTool()
versions_tool = ListVersionsTool()
compare_tool = CompareVersionsTool()
tests_tool = RunTestsTool()
retrain_tool = TriggerRetrainTool()
status_tool = CheckRetrainStatusTool()


# ────── 4. Gradio UI Functions ──────

def refresh_status():
    try:
        # 1) Detect drift
        drift_payload = {"agent_name": "chat_bot", "time_window": "7d", "drift_type": "semantic"}
        drift_data = drift_tool.run(drift_payload)
        drift_text = f"Drift Score: {drift_data.get('drift_score', 'N/A')} ({drift_data.get('status', 'unknown')})"

        if drift_data.get("status") == "alert":
            drift_summary = summarize_text(str(drift_data), "Drift Analysis")
        else:
            drift_summary = "✅ No significant drift detected. System performing within normal parameters."

        # 2) List versions
        versions = versions_tool.run()
        version_text = f"📋 Available Versions ({len(versions)}): " + ", ".join(versions)

        return drift_text, drift_summary, version_text
    except Exception as e:
        logger.error(f"Error in refresh_status: {e}")
        return f"❌ Error: {str(e)}", "Failed to get drift summary", "Failed to get versions"


def run_tests_and_summarize(version):
    if not version:
        return "⚠️ Please select a version", "No version selected for testing"

    try:
        test_payload = {"agent_version": version}
        test_results = tests_tool.run(test_payload)

        # Format test results
        test_text = "🧪 Test Results:\n" + "\n".join([f"  • {k}: {v}" for k, v in test_results.items()])

        # Generate summary
        test_summary = summarize_text(str(test_results), "Test Summary")

        # Add status indicators
        passed_count = sum(1 for status in test_results.values() if status == "passed")
        total_count = len(test_results)
        test_summary += f"\n\n📊 Overall: {passed_count}/{total_count} tests passed"

        return test_text, test_summary
    except Exception as e:
        logger.error(f"Error in run_tests_and_summarize: {e}")
        return f"❌ Error: {str(e)}", "Failed to get test summary"


def compare_versions(v_left, v_right):
    if not v_left or not v_right:
        return "⚠️ Please select both versions to compare"

    if v_left == v_right:
        return "⚠️ Please select different versions to compare"

    try:
        compare_payload = {"left": v_left, "right": v_right}
        diff = compare_tool.run(compare_payload)
        diffs = diff.get("metric_differences", {})

        # Format the comparison nicely
        comparison_lines = []
        for metric, value in diffs.items():
            if isinstance(value, (int, float)):
                direction = "📈" if value > 0 else "📉" if value < 0 else "➡️"
                comparison_lines.append(f"  {direction} {metric.title()}: {value:+.3f}")

        result = f"📊 Comparison ({v_left} vs {v_right}):\n" + "\n".join(comparison_lines)
        return result
    except Exception as e:
        logger.error(f"Error in compare_versions: {e}")
        return f"❌ Error: {str(e)}"


def trigger_retraining(version):
    if not version:
        return "⚠️ Please select a version"

    try:
        retrain_payload = {"base_version": version}
        job = retrain_tool.run(retrain_payload)
        job_id = job.get("job_id", "Unknown job ID")
        return f"🚀 Retraining initiated: {job_id}"
    except Exception as e:
        logger.error(f"Error in trigger_retraining: {e}")
        return f"❌ Error: {str(e)}"


def check_retrain_status(job_id):
    if not job_id or job_id.startswith("⚠️") or job_id.startswith("❌"):
        return "⚠️ No valid job ID provided"

    try:
        status_payload = {"job_id": job_id}
        status = status_tool.run(status_payload)

        current_status = status.get("status", "unknown")
        progress = status.get("progress", "N/A")

        if current_status == "done":
            new_v = status.get("new_version", f"v{job_id[-4:]}")
            # Automatically store new version
            try:
                requests.post("http://localhost:6003/store_version", json={
                    "new_version": new_v,
                    "base_version": job_id,
                    "model_uri": f"s3://bucket/{new_v}",
                    "prompt_config": {},
                    "notes": "Automatically retrained"
                }, timeout=5)
            except Exception as store_error:
                logger.error(f"Error storing version: {store_error}")

            return f"✅ Retraining complete! New version: {new_v}"
        elif current_status == "failed":
            return f"❌ Retraining failed. Please check logs."
        else:
            stage = status.get("current_stage", "processing")
            eta = status.get("estimated_completion", "unknown")
            return f"🔄 Status: {current_status.title()} ({progress})\n📍 Stage: {stage}\n⏱️ ETA: {eta}"
    except Exception as e:
        logger.error(f"Error in check_retrain_status: {e}")
        return f"❌ Error: {str(e)}"


# ────── 5. Gradio UI Definition ──────

def create_dashboard():
    # Get initial version list for dropdowns
    try:
        available_versions = versions_tool.run()
    except:
        available_versions = ["v1.0.0", "v1.1.0", "v2.0.0"]

    with gr.Blocks(
            title="AI Agent Lifecycle Monitor",
            theme=gr.themes.Soft(),
            css="""
        .gradio-container {
            max-width: 1200px !important;
        }
        .status-box {
            border-radius: 10px;
            padding: 15px;
        }
        """
    ) as demo:
        gr.Markdown("""
        # 🤖 AI Agent Lifecycle Monitor Dashboard
        ### Real-time monitoring and management for AI agent deployments
        """)

        # Status Section
        with gr.Group():
            gr.Markdown("## 📊 System Status")
            with gr.Row():
                refresh_btn = gr.Button("🔄 Refresh Status", variant="primary", scale=1)

            with gr.Row():
                with gr.Column():
                    drift_out = gr.Textbox(
                        label="🌊 Drift Detection",
                        interactive=False,
                        elem_classes=["status-box"]
                    )
                with gr.Column():
                    drift_summary_out = gr.Textbox(
                        label="📝 Drift Analysis",
                        interactive=False,
                        lines=3,
                        elem_classes=["status-box"]
                    )

            version_out = gr.Textbox(
                label="📦 Version Inventory",
                interactive=False,
                elem_classes=["status-box"]
            )

        refresh_btn.click(
            fn=refresh_status,
            inputs=[],
            outputs=[drift_out, drift_summary_out, version_out]
        )

        # Testing Section
        with gr.Group():
            gr.Markdown("## 🧪 Testing & Quality Assurance")
            with gr.Row():
                version_input = gr.Dropdown(
                    choices=available_versions,
                    label="Select Version for Testing",
                    value=available_versions[0] if available_versions else None,
                    scale=2
                )
                run_tests_btn = gr.Button("▶️ Run Tests", variant="secondary", scale=1)

            with gr.Row():
                with gr.Column():
                    tests_out = gr.Textbox(
                        label="Test Results",
                        interactive=False,
                        lines=6,
                        elem_classes=["status-box"]
                    )
                with gr.Column():
                    test_summary_out = gr.Textbox(
                        label="Test Summary & Recommendations",
                        interactive=False,
                        lines=6,
                        elem_classes=["status-box"]
                    )

        run_tests_btn.click(
            fn=run_tests_and_summarize,
            inputs=[version_input],
            outputs=[tests_out, test_summary_out]
        )

        # Version Comparison Section
        with gr.Group():
            gr.Markdown("## ⚖️ Version Comparison")
            with gr.Row():
                left_ver = gr.Dropdown(
                    choices=available_versions,
                    label="Base Version",
                    value=available_versions[0] if available_versions else None
                )
                right_ver = gr.Dropdown(
                    choices=available_versions,
                    label="Compare Version",
                    value=available_versions[1] if len(available_versions) > 1 else None
                )
                compare_btn = gr.Button("🔍 Compare", variant="secondary")

            compare_out = gr.Textbox(
                label="Comparison Results",
                interactive=False,
                lines=4,
                elem_classes=["status-box"]
            )

        compare_btn.click(
            fn=compare_versions,
            inputs=[left_ver, right_ver],
            outputs=[compare_out]
        )

        # Retraining Section
        with gr.Group():
            gr.Markdown("## 🔄 Model Retraining")
            with gr.Row():
                retrain_ver = gr.Dropdown(
                    choices=available_versions,
                    label="Base Version for Retraining",
                    value=available_versions[0] if available_versions else None,
                    scale=2
                )
                retrain_btn = gr.Button("🚀 Start Retraining", variant="primary", scale=1)

            job_id_out = gr.Textbox(
                label="Retraining Job Status",
                interactive=False,
                elem_classes=["status-box"]
            )

            with gr.Row():
                check_btn = gr.Button("📊 Check Progress", variant="secondary")
                retrain_status_out = gr.Textbox(
                    label="Detailed Status",
                    interactive=False,
                    lines=3,
                    elem_classes=["status-box"]
                )

        retrain_btn.click(
            fn=trigger_retraining,
            inputs=[retrain_ver],
            outputs=[job_id_out]
        )

        check_btn.click(
            fn=check_retrain_status,
            inputs=[job_id_out],
            outputs=[retrain_status_out]
        )

        # Footer
        gr.Markdown("""
        ---
        💡 **Note**: This dashboard provides fallback data when backend services are unavailable.
        All network requests include timeout protection for better reliability.
        """)

    return demo


def launch_dashboard():
    try:
        demo = create_dashboard()
        logger.info("Starting dashboard server...")
        demo.launch(
            server_name="0.0.0.0",
            server_port=7860,
            share=False,
            show_error=True,
            quiet=False
        )
    except Exception as e:
        logger.error(f"Error launching dashboard: {e}")
        print(f"Failed to launch dashboard: {e}")


if __name__ == "__main__":
    launch_dashboard()