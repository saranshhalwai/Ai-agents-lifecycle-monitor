# dashboard/gradio_dashboard.py
import asyncio
import signal
from contextlib import AsyncExitStack

import gradio as gr
from transformers import pipeline
import requests
from typing import Dict, List, Any, Optional
import logging
from mcp.client.stdio import stdio_client
from mcp import ClientSession, StdioServerParameters

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 1. Set up an open-source summarization LLM (Flan-T5-Small) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
try:
    summarizer_hf = pipeline(
        "summarization",
        model="google/flan-t5-small",
        device=-1,  # CPU only
        framework="pt",  # PyTorch
        max_length=50,  # Reduced to avoid warnings
        min_length=10,  # Reduced proportionally
        do_sample=False,
    )
    logger.info("LLM pipeline initialized successfully")
except Exception as e:
    logger.error(f"Error initializing LLM: {e}")
    summarizer_hf = None


# Simple summarization function to replace LangChain
def summarize_text(text: str, context: str = "") -> str:
    """Simple text summarization function"""
    if summarizer_hf is not None:
        try:
            # Truncate input if too long
            max_input_length = 200
            if len(text) > max_input_length:
                text = text[:max_input_length] + "..."

            result = summarizer_hf(text, max_length=30, min_length=10, do_sample=False)
            summary = result[0]['summary_text']
            return f"{context}: {summary}" if context else summary
        except Exception as e:
            logger.error(f"Error in summarization: {e}")
            return f"{context}: Error generating summary - {str(e)[:100]}..."
    else:
        return f"{context}: Mock summary for: {text[:100]}..."


class MCPClient:
    def __init__(self, name):
        self.session: Optional[ClientSession] = None
        self.exit_stack = AsyncExitStack()
        self.name = name

    async def connect(self, server_script_path: str):
        """Connect to an MCP server using a script path."""
        server_params = StdioServerParameters(
            command="python",
            args=[server_script_path],
            env=None
        )
        stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))
        self.stdio, self.write = stdio_transport
        self.session = await self.exit_stack.enter_async_context(ClientSession(self.stdio, self.write))
        await self.session.initialize()
        logger.info(f"Connected to {self.name} server")

    async def list_tools(self):
        response = await self.session.list_tools()
        tools = response.tools
        return tools

    async def cleanup(self):
        """Clean up resources"""
        await self.exit_stack.aclose()


# Global clients and shutdown event
drift_client = MCPClient("DriftMonitor")
versions_client = MCPClient("VersionsMonitor")
tests_client = MCPClient("TestRunner")
retrain_client = MCPClient("RetrainingServer")

shutdown_event = asyncio.Event()


async def connect_all():
    """Background task to maintain MCP connections"""
    try:
        await asyncio.gather(
            drift_client.connect("servers/drift_monitor_server.py"),
            versions_client.connect("servers/version_control_server.py"),
            tests_client.connect("servers/test_runner_server.py"),
            retrain_client.connect("servers/retraining.py")
        )
        logger.info("All MCP clients connected successfully!")

        # Keep connections alive until shutdown
        await shutdown_event.wait()

    except Exception as e:
        logger.error(f"Error in MCP connections: {e}")
        raise
    finally:
        logger.info("Cleaning up MCP connections...")
        await disconnect_all()


async def disconnect_all():
    """Clean up all MCP client connections"""
    try:
        await asyncio.gather(
            drift_client.cleanup(),
            versions_client.cleanup(),
            tests_client.cleanup(),
            retrain_client.cleanup(),
            return_exceptions=True
        )
        logger.info("All MCP clients disconnected")
    except Exception as e:
        logger.error(f"Error during cleanup: {e}")


def signal_handler():
    """Handle shutdown signals gracefully"""
    logger.info("Received shutdown signal...")
    shutdown_event.set()


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 2. Tools for MCP Endpoints ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

class DetectDriftTool:
    def __init__(self):
        self.name = "detect_drift"
        self.description = "Detect semantic drift for a given agent over a time window."

    def run(self, tool_input: Dict[str, Any]) -> Dict[str, Any]:
        try:
            resp = requests.post("http://localhost:7002/detect_drift", json=tool_input, timeout=5)
            resp.raise_for_status()
            return resp.json()
        except Exception as e:
            logger.error(f"Error detecting drift: {e}")
            return {
                "drift_score": 0.15,
                "status": "normal",
                "details": "Fallback data - service unavailable",
                "timestamp": "2025-06-03T19:30:00Z"
            }


class ListVersionsTool:
    def __init__(self):
        self.name = "list_versions"
        self.description = "Retrieve the list of stored agent versions."

    def run(self, tool_input: Dict[str, Any] = None) -> List[str]:
        try:
            resp = requests.get("http://localhost:7003/list_versions", timeout=5)
            resp.raise_for_status()
            return resp.json()
        except Exception as e:
            logger.error(f"Error listing versions: {e}")
            return ["v1.0.0", "v1.1.0", "v2.0.0", "v2.1.0"]  # Fallback versions


class CompareVersionsTool:
    def __init__(self):
        self.name = "compare_versions"
        self.description = "Compare two agent versions and return metric diffs."

    def run(self, tool_input: Dict[str, Any]) -> Dict[str, Any]:
        try:
            resp = requests.post("http://localhost:7003/compare_versions", json=tool_input, timeout=5)
            resp.raise_for_status()
            return resp.json()
        except Exception as e:
            logger.error(f"Error comparing versions: {e}")
            return {
                "metric_differences": {
                    "accuracy": 0.035,
                    "latency": -0.12,
                    "memory_usage": 0.08,
                    "throughput": 0.05
                },
                "summary": "Fallback comparison data"
            }


class RunTestsTool:
    def __init__(self):
        self.name = "run_tests"
        self.description = "Run all tests on a given agent version."

    def run(self, tool_input: Dict[str, Any]) -> Dict[str, str]:
        try:
            tests = requests.get("http://localhost:7001/list_tests", timeout=5).json()
            results = {}
            for test_name in tests:
                payload = {"test_name": test_name, "agent_version": tool_input["agent_version"]}
                r = requests.post("http://localhost:7001/run_test", json=payload, timeout=10)
                r.raise_for_status()
                results[test_name] = r.json()["status"]
            return results
        except Exception as e:
            logger.error(f"Error running tests: {e}")
            return {
                "unit_tests": "passed",
                "integration_tests": "passed",
                "performance_tests": "warning",
                "security_tests": "passed",
                "regression_tests": "failed"
            }


class TriggerRetrainTool:
    def __init__(self):
        self.name = "trigger_retraining"
        self.description = "Trigger retraining via Modal."

    def run(self, tool_input: Dict[str, Any]) -> Dict[str, str]:
        try:
            resp = requests.post("http://localhost:7004/trigger_retraining", json=tool_input, timeout=5)
            resp.raise_for_status()
            return resp.json()
        except Exception as e:
            logger.error(f"Error triggering retraining: {e}")
            job_id = f"retrain_job_{hash(str(tool_input)) % 10000}"
            return {"job_id": job_id, "status": "initiated", "message": "Mock retraining job started"}


class CheckRetrainStatusTool:
    def __init__(self):
        self.name = "check_retraining_status"
        self.description = "Check the status of an ongoing retraining job."

    def run(self, tool_input: Dict[str, Any]) -> Dict[str, str]:
        try:
            resp = requests.post("http://localhost:7004/check_retraining_status", json=tool_input, timeout=5)
            resp.raise_for_status()
            return resp.json()
        except Exception as e:
            logger.error(f"Error checking retraining status: {e}")
            return {
                "status": "running",
                "progress": "65%",
                "estimated_completion": "15 minutes",
                "current_stage": "model_validation"
            }


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 3. Initialize Tools ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
drift_tool = DetectDriftTool()
versions_tool = ListVersionsTool()
compare_tool = CompareVersionsTool()
tests_tool = RunTestsTool()
retrain_tool = TriggerRetrainTool()
status_tool = CheckRetrainStatusTool()


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 4. Gradio UI Functions ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def refresh_status():
    try:
        # 1) Detect drift
        drift_payload = {"agent_name": "chat_bot", "time_window": "7d", "drift_type": "semantic"}
        drift_data = drift_tool.run(drift_payload)
        drift_text = f"Drift Score: {drift_data.get('drift_score', 'N/A')} ({drift_data.get('status', 'unknown')})"

        if drift_data.get("status") == "alert":
            drift_summary = summarize_text(str(drift_data), "Drift Analysis")
        else:
            drift_summary = "‚úÖ No significant drift detected. System performing within normal parameters."

        # 2) List versions
        versions = versions_tool.run()
        version_text = f"üìã Available Versions ({len(versions)}): " + ", ".join(versions)

        return drift_text, drift_summary, version_text
    except Exception as e:
        logger.error(f"Error in refresh_status: {e}")
        return f"‚ùå Error: {str(e)}", "Failed to get drift summary", "Failed to get versions"


def run_tests_and_summarize(version):
    if not version:
        return "‚ö†Ô∏è Please select a version", "No version selected for testing"

    try:
        test_payload = {"agent_version": version}
        test_results = tests_tool.run(test_payload)

        # Format test results
        test_text = "üß™ Test Results:\n" + "\n".join([f"  ‚Ä¢ {k}: {v}" for k, v in test_results.items()])

        # Generate summary
        test_summary = summarize_text(str(test_results), "Test Summary")

        # Add status indicators
        passed_count = sum(1 for status in test_results.values() if status == "passed")
        total_count = len(test_results)
        test_summary += f"\n\nüìä Overall: {passed_count}/{total_count} tests passed"

        return test_text, test_summary
    except Exception as e:
        logger.error(f"Error in run_tests_and_summarize: {e}")
        return f"‚ùå Error: {str(e)}", "Failed to get test summary"


def compare_versions(v_left, v_right):
    if not v_left or not v_right:
        return "‚ö†Ô∏è Please select both versions to compare"

    if v_left == v_right:
        return "‚ö†Ô∏è Please select different versions to compare"

    try:
        compare_payload = {"left": v_left, "right": v_right}
        diff = compare_tool.run(compare_payload)
        diffs = diff.get("metric_differences", {})

        # Format the comparison nicely
        comparison_lines = []
        for metric, value in diffs.items():
            if isinstance(value, (int, float)):
                direction = "üìà" if value > 0 else "üìâ" if value < 0 else "‚û°Ô∏è"
                comparison_lines.append(f"  {direction} {metric.title()}: {value:+.3f}")

        result = f"üìä Comparison ({v_left} vs {v_right}):\n" + "\n".join(comparison_lines)
        return result
    except Exception as e:
        logger.error(f"Error in compare_versions: {e}")
        return f"‚ùå Error: {str(e)}"


def trigger_retraining(version):
    if not version:
        return "‚ö†Ô∏è Please select a version"

    try:
        retrain_payload = {"base_version": version}
        job = retrain_tool.run(retrain_payload)
        job_id = job.get("job_id", "Unknown job ID")
        return f"üöÄ Retraining initiated: {job_id}"
    except Exception as e:
        logger.error(f"Error in trigger_retraining: {e}")
        return f"‚ùå Error: {str(e)}"


def check_retrain_status(job_id):
    if not job_id or job_id.startswith("‚ö†Ô∏è") or job_id.startswith("‚ùå"):
        return "‚ö†Ô∏è No valid job ID provided"

    try:
        status_payload = {"job_id": job_id}
        status = status_tool.run(status_payload)

        current_status = status.get("status", "unknown")
        progress = status.get("progress", "N/A")

        if current_status == "done":
            new_v = status.get("new_version", f"v{job_id[-4:]}")
            # Automatically store new version
            try:
                requests.post("http://localhost:7003/store_version", json={
                    "new_version": new_v,
                    "base_version": job_id,
                    "model_uri": f"s3://bucket/{new_v}",
                    "prompt_config": {},
                    "notes": "Automatically retrained"
                }, timeout=5)
            except Exception as store_error:
                logger.error(f"Error storing version: {store_error}")

            return f"‚úÖ Retraining complete! New version: {new_v}"
        elif current_status == "failed":
            return f"‚ùå Retraining failed. Please check logs."
        else:
            stage = status.get("current_stage", "processing")
            eta = status.get("estimated_completion", "unknown")
            return f"üîÑ Status: {current_status.title()} ({progress})\nüìç Stage: {stage}\n‚è±Ô∏è ETA: {eta}"
    except Exception as e:
        logger.error(f"Error in check_retrain_status: {e}")
        return f"‚ùå Error: {str(e)}"


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 5. Gradio UI Definition ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def create_dashboard():
    # Get initial version list for dropdowns
    try:
        available_versions = versions_tool.run()
    except:
        available_versions = ["v1.0.0", "v1.1.0", "v2.0.0"]

    with gr.Blocks(
            title="AI Agent Lifecycle Monitor",
            theme=gr.themes.Soft(),
            css="""
        .gradio-container {
            max-width: 1200px !important;
        }
        .status-box {
            border-radius: 10px;
            padding: 15px;
        }
        """
    ) as demo:
        gr.Markdown("""
        # ü§ñ AI Agent Lifecycle Monitor Dashboard
        ### Real-time monitoring and management for AI agent deployments
        """)

        # Status Section
        with gr.Group():
            gr.Markdown("## üìä System Status")
            with gr.Row():
                refresh_btn = gr.Button("üîÑ Refresh Status", variant="primary", scale=1)

            with gr.Row():
                with gr.Column():
                    drift_out = gr.Textbox(
                        label="üåä Drift Detection",
                        interactive=False,
                        elem_classes=["status-box"]
                    )
                with gr.Column():
                    drift_summary_out = gr.Textbox(
                        label="üìù Drift Analysis",
                        interactive=False,
                        lines=3,
                        elem_classes=["status-box"]
                    )

            version_out = gr.Textbox(
                label="üì¶ Version Inventory",
                interactive=False,
                elem_classes=["status-box"]
            )

        refresh_btn.click(
            fn=refresh_status,
            inputs=[],
            outputs=[drift_out, drift_summary_out, version_out]
        )

        # Testing Section
        with gr.Group():
            gr.Markdown("## üß™ Testing & Quality Assurance")
            with gr.Row():
                version_input = gr.Dropdown(
                    choices=available_versions,
                    label="Select Version for Testing",
                    value=available_versions[0] if available_versions else None,
                    scale=2
                )
                run_tests_btn = gr.Button("‚ñ∂Ô∏è Run Tests", variant="secondary", scale=1)

            with gr.Row():
                with gr.Column():
                    tests_out = gr.Textbox(
                        label="Test Results",
                        interactive=False,
                        lines=6,
                        elem_classes=["status-box"]
                    )
                with gr.Column():
                    test_summary_out = gr.Textbox(
                        label="Test Summary & Recommendations",
                        interactive=False,
                        lines=6,
                        elem_classes=["status-box"]
                    )

        run_tests_btn.click(
            fn=run_tests_and_summarize,
            inputs=[version_input],
            outputs=[tests_out, test_summary_out]
        )

        # Version Comparison Section
        with gr.Group():
            gr.Markdown("## ‚öñÔ∏è Version Comparison")
            with gr.Row():
                left_ver = gr.Dropdown(
                    choices=available_versions,
                    label="Base Version",
                    value=available_versions[0] if available_versions else None
                )
                right_ver = gr.Dropdown(
                    choices=available_versions,
                    label="Compare Version",
                    value=available_versions[1] if len(available_versions) > 1 else None
                )
                compare_btn = gr.Button("üîç Compare", variant="secondary")

            compare_out = gr.Textbox(
                label="Comparison Results",
                interactive=False,
                lines=4,
                elem_classes=["status-box"]
            )

        compare_btn.click(
            fn=compare_versions,
            inputs=[left_ver, right_ver],
            outputs=[compare_out]
        )

        # Retraining Section
        with gr.Group():
            gr.Markdown("## üîÑ Model Retraining")
            with gr.Row():
                retrain_ver = gr.Dropdown(
                    choices=available_versions,
                    label="Base Version for Retraining",
                    value=available_versions[0] if available_versions else None,
                    scale=2
                )
                retrain_btn = gr.Button("üöÄ Start Retraining", variant="primary", scale=1)

            job_id_out = gr.Textbox(
                label="Retraining Job Status",
                interactive=False,
                elem_classes=["status-box"]
            )

            with gr.Row():
                check_btn = gr.Button("üìä Check Progress", variant="secondary")
                retrain_status_out = gr.Textbox(
                    label="Detailed Status",
                    interactive=False,
                    lines=3,
                    elem_classes=["status-box"]
                )

        retrain_btn.click(
            fn=trigger_retraining,
            inputs=[retrain_ver],
            outputs=[job_id_out]
        )

        check_btn.click(
            fn=check_retrain_status,
            inputs=[job_id_out],
            outputs=[retrain_status_out]
        )

        # Footer
        gr.Markdown("""
        ---
        üí° **Note**: This dashboard provides fallback data when backend services are unavailable.
        All network requests include timeout protection for better reliability.
        """)

    return demo


async def main():
    """Main async function to coordinate MCP connections and Gradio dashboard"""
    # Set up graceful shutdown
    loop = asyncio.get_running_loop()
    for sig in (signal.SIGTERM, signal.SIGINT):
        try:
            loop.add_signal_handler(sig, signal_handler)
        except NotImplementedError:
            # Windows doesn't support add_signal_handler
            pass

    # Start MCP connection manager as background task
    logger.info("Starting MCP connections...")
    connection_task = asyncio.create_task(connect_all())

    # Give connections time to establish
    await asyncio.sleep(2)

    try:
        # Create and launch Gradio dashboard
        demo = create_dashboard()
        logger.info("Starting Gradio dashboard server...")

        # Launch in a way that doesn't block the event loop
        demo.launch(
            server_name="0.0.0.0",
            server_port=7860,
            share=False,
            show_error=True,
            quiet=False,
            prevent_thread_lock=True  # This allows the event loop to continue
        )

        # Keep the main function running
        while not shutdown_event.is_set():
            await asyncio.sleep(1)

    except KeyboardInterrupt:
        logger.info("Received keyboard interrupt...")
        shutdown_event.set()
    except Exception as e:
        logger.error(f"Error in main: {e}")
        shutdown_event.set()
    finally:
        # Wait for connection cleanup
        logger.info("Waiting for MCP connections to cleanup...")
        try:
            await asyncio.wait_for(connection_task, timeout=5.0)
        except asyncio.TimeoutError:
            logger.warning("MCP cleanup timed out")

        logger.info("Application shutdown complete")


def launch_dashboard():
    """Entry point that handles both sync and async execution"""
    try:
        # Run the async main function
        asyncio.run(main(), debug=True)
    except Exception as e:
        logger.error(f"Error launching dashboard: {e}")
        print(f"Failed to launch dashboard: {e}")


if __name__ == "__main__":
    launch_dashboard()